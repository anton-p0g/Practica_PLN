---
title: "Tercera parte de la entrega"
output: pdf_document
date: "2024-11-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
```

## Objetivos de la Tercera Parte

En esta parte del proyecto, se nos pide implementar un programa llamado `train_analysis.R` que realice el análisis del corpus obtenido en la primera parte, almacenado en el archivo `spanish_train.qcorpus.rds`. El programa debe calcular, para cada documento, lo siguiente:

1. **Frecuencia de verbos del título**: Contar cuántos verbos, convertidos a su forma en infinitivo, presentes en el campo `title` del documento también aparecen en el texto completo correspondiente.
2. **Frecuencia de verbos del resumen**: Contar cuántos verbos, convertidos a su forma en infinitivo, presentes en el campo `summary` del documento también aparecen en el texto completo correspondiente.


Estos resultados se presentan de manera visual mediante dos histogramas:
- Un histograma para los verbos del `title`.
- Otro histograma para los verbos del `summary`.

En los histogramas obtenidos, se muestra la distribución de las frecuencias de coincidencia y el número de documentos que corresponden a cada frecuencia. Esto permite visualizar de forma clara cómo se relacionan los verbos presentes en los `title` y `summary` con el contenido completo de los textos.


---

## Importación de Código para Tests

Para mantener el documento más organizado y legible, importamos el código de tests desde otro archivo. Esto nos permite ejecutar y mostrar los resultados de las pruebas específicas de las funciones utilizadas en el programa principal sin añadir bloques de código extensos.

La importación se realiza con la función `source()` de R, que permite cargar y ejecutar el contenido de un archivo externo. En este caso, importamos el archivo `test_train_analysis.R`, donde se encuentran definidas las pruebas.


```{r include=FALSE}
source("../pruebas/test_train_analysis.R")
```

---

## Uso de Archivos RDS para Optimizar el Procesamiento

Dado que el parseo de textos con la función `spacy_parse()` puede ser un proceso lento y costoso, decidimos implementar una estructura `if-else` para gestionar el almacenamiento y la carga de los datos procesados. La idea principal es evitar realizar el parseo en cada ejecución, almacenando los resultados en archivos `.RDS`, que son compactos y rápidos de cargar.


### Estructura General

La estructura que utilizamos funciona de la siguiente manera:

- **Si el archivo RDS ya existe**:
  - Cargamos directamente los datos procesados con la función `readRDS()`.
  - Esto reduce significativamente el tiempo de ejecución en futuras sesiones.
  
- **Si el archivo RDS no existe**:
  - Parseamos los datos correspondientes con `spacy_parse()` y los guardamos en un archivo `.RDS` utilizando `saveRDS()`.
  - Esto asegura que el procesamiento solo se realiza la primera vez.


### Función `spacy_parse()`

1. **Tokenización**:
   - Divide el texto en **tokens** (en nuestro caso las palabras de los textos).
   
2. **Etiquetado gramatical (POS tagging)**:
   - Asocia cada token con su categoría gramatical (por ejemplo, `VERB`, `NOUN`, `ADJ`).
   
3. **Lematización**:
   - Convierte las palabras a su forma raíz (por ejemplo, "corriendo" → "correr").


El propósito principal de usar `spacy_parse()` en este proyecto es que nos permita trabajar específicamente con los **verbos** en infinitivo de los textos. Esto incluye:

- Identificar todos los verbos en un texto (`pos == "VERB"`).
- Obtener los verbos en su forma **lema** (`lemma`), para evitar inconsistencias debido a conjugaciones.

## Resultado de `spacy_parse()`

Tras realizar el `spacy_parse()` sobre las palabras de los campos `title`, `summary` y los textos completos, obtenemos una tabla estructurada que contiene información clave sobre los tokens del texto. En nuestro caso, nos quedamos con las columnas `lemma` (forma en infinitivo de los verbos) y `doc_id` (identificador único de cada documento).

El uso del `doc_id` es bastante importante, ya que nos permite asociar cada verbo con su documento correspondiente.

### Ejemplo de los Resultados

Verbos del campo `title`:

```{r echo=FALSE}
head(verbs_title)
```
Verbos del campo `summary`:

```{r echo=FALSE}
head(verbs_summary)
```

Verbos de los textos completos:

```{r echo=FALSE}
head(verbs_text)
```
---

## Función `verbs_create_list`

La función `verbs_create_list` organiza los verbos extraídos del corpus en una estructura más manejable, agrupándolos por documento. Su objetivo principal es asociar cada verbo a su documento correspondiente, utilizando el identificador único de cada documento (`doc_id`).

La función toma los verbos extraídos de un campo del corpus, como `title` o `summary`, y los organiza en una lista, donde cada elemento corresponde a un documento específico. Si un documento no contiene verbos en un campo determinado, se incluye en la lista con un vector vacío, asegurando que todos los documentos estén representados de manera consistente.

El resultado de esta función es una lista con nombres que corresponden a los `doc_id` del corpus y cuyo contenido son los verbos en su forma lematizada. Esta estructura facilita el acceso y análisis de los datos, permitiéndonos comparar los verbos presentes en los títulos o resúmenes con los del texto completo de cada documento.

Resultados tras aplicar la función sobre los dataframes obtenidos anteriormente:

Aplicando para `verbs_titles`:

```{r echo=FALSE}
head(list_verbs_titles)
```

Para `verbs_summary`:

```{r echo=FALSE}
head(list_verbs_summary, 3)
```

Lo mismo con `verbs_text`:

```{r echo=FALSE}
print(list_verbs_text[1:3])
```


## Eliminación de Verbos Duplicados

Como indica el enunciado, necesitamos trabajar con los **verbos únicos** de los campos `title` y `summary`, ya que no queremos contar verbos repetidos dentro de un mismo documento. Para lograr esto, definimos la función `make_unique()`, que elimina duplicados en cada elemento de una lista. Su objetivo es garantizar que cada sublista contenga solo valores **únicos**.

La función recorre cada elemento de la lista (correspondiente a un documento) y, si este no está vacío, utiliza la función `unique()` para filtrar los valores duplicados. De esta forma, el resultado es una lista en la que cada sublista contiene únicamente los verbos únicos asociados a su documento.

### Ejemplo de Uso

Si observamos el contenido original de `list_verbs_summary`, en el documento **text_5** aparece el verbo `tener` dos veces. 

```{r echo=FALSE}
list_verbs_summary[5]
```
Después de aplicar `make_unique()`, el resultado muestra este verbo solo una vez.

```{r echo=FALSE}
make_unique(list_verbs_summary[5])
```
---

## Cálculo de Frecuencias de Verbos

Llegamos al núcleo del proyecto: **contar cuántas veces los verbos presentes en los campos `title` y `summary` aparecen en el texto completo de cada documento**. Para ello, definimos la función `freq_verbs`, que calcula la frecuencia total de los verbos en el texto correspondiente.

### Explicación de la Función `freq_verbs`

La función `freq_verbs` toma tres argumentos:

1. **`text_list`**: Lista de verbos en los textos completos.
2. **`list_comp`**: Lista de verbos a comparar (pueden ser de `title` o `summary`).
3. **`corpus_ids`**: Identificadores únicos de los documentos.

El objetivo de esta función es recorrer los textos y contar cuántas veces los verbos presentes en `list_comp` aparecen en el texto completo correspondiente. Para ello:

- Se inicializa un vector vacío llamado `freq`, cuya longitud corresponde al número de documentos, y se asignan los `corpus_ids` como nombres.
- Para cada documento, se recorren los verbos de `list_comp` y se cuenta cuántas veces aparecen en el texto completo (`text_list`) utilizando `sum()`.
- Finalmente, el vector `freq` almacena la frecuencia total de los verbos por documento y se devuelve como resultado.


Aplicamos esta función y obtenemos las frecuencias de los verbos de cada `title` 
en su texto y de cada `summary` en su texto.

Veamos las frecuencias de los `titles` 50 - 100:

```{r}
freq_verbs_titles[50:100]
```

Veamos las frecuencias de los `summary` 50 - 100:

```{r}
freq_verbs_summary[50:100]
```

Ahora viene la parte de los **testeos** para confirmar que lo que hemos hecho es
correcto. Para realizarlo, vamos a crear otra función `test_freq_verbs`.

La función `test_freq_verbs` prueba la función `freq_verbs` en un **subconjunto** de documentos, 
calculando la frecuencia de verbos en títulos y resúmenes. Muestra los verbos
coincidentes entre texto, título y resumen, y sus frecuencias correspondientes para
cada documento.

Veamos el testeo:

```{r, results = "asis"}
test_freq_verbs(list_verbs_text, list_verbs_title_unique, list_verbs_summary_unique, corpus_ids)
```

Gráficamente podemos **visualizar** esto. Primero, vamos a crear una **tabla** con
las frecuencias de los resumenes y hacer un `bar_plot`. Quedando lo siguiente:

```{r echo=FALSE}
freq_table1 <- table(freq_verbs_summary)

bar_midpoints <- barplot(freq_table1, 
                         main = "Frequency of Matched Verbs Between Texts and Summaries", 
                         xlab = "Frequency", 
                         ylab = "Number of Documents", 
                         col = "lightblue", 
                         border = "black",
                         ylim = c(0, max(freq_table1) + 1500), 
                         yaxt = "n", 
                         xaxt = "n") # Suppress both axes

y_max <- max(freq_table1) + 2000
y_ticks <- seq(0, y_max, by = 2000) # y-axis subdivisions
axis(2, at = y_ticks, labels = y_ticks, las = 1, cex.axis = 0.8)

x_labels <- as.numeric(names(freq_table1))
axis(1, at = bar_midpoints, labels = x_labels, las = 2, cex.axis = 0.5) # Rotate labels vertically

if (length(bar_midpoints) == length(x_labels)) {
  axis(1, at = bar_midpoints[length(bar_midpoints)], 
       labels = x_labels[length(x_labels)], las = 2, cex.axis = 0.5)
}

# Add labels above all bars
text(bar_midpoints, freq_table1, 
     labels = freq_table1, 
     pos = 3, srt = 45, cex = 0.4, col = "black")
```

Y la **tabla de frecuencias**: 

```{r}
freq_table1
```
También podemos hacer esto con los **títulos** y quedaría lo siguiente:

```{r echo=FALSE}
freq_table2 <- table(freq_verbs_titles)
  
bar_midpoints <- barplot(freq_table2, 
                           main = "Frequency of Matched Verbs Between Texts and Titles", 
                           xlab = "Frequency", 
                           ylab = "Number of Documents", 
                           col = "lightblue", 
                           border = "black", 
                           ylim = c(0, max(freq_table2) + 1500),
                           yaxt = "n",
                           xaxt = "n")
  
  y_max <- max(freq_table2) + 2000 # Defines the maximum y-axis value
  y_ticks <- seq(0, y_max, by = 1500) # subdivisions
  axis(2, at = y_ticks, labels = y_ticks, las = 1, cex.axis = 0.8)
  
  x_labels <- as.numeric(names(freq_table2))
  axis(1, at = bar_midpoints, labels = x_labels, las = 2, cex.axis = 0.5) # Rotate labels vertically
  
  if (length(bar_midpoints) == length(x_labels)) {
    axis(1, at = bar_midpoints[length(bar_midpoints)], 
         labels = x_labels[length(x_labels)], las = 2, cex.axis = 0.5)
  }

  # Add labels above all bars
  text(bar_midpoints, freq_table2, 
       labels = freq_table2, 
       pos = 3, srt = 45, cex = 0.4, col = "black")
```

Con su **tabla de frecuencias**:

```{r}
freq_table2
```


Ahora hagamos un testeo para una determinada **frecuencia de summary**. Para ello, 
creamos la función `test_specific_frequency_summary`.

La función `test_specific_frequency_summary` busca documentos con una frecuencia 
**específica** de verbos en los **resúmenes**, muestra detalles de los verbos coincidentes
entre texto y resumen, y presenta el **número de coincidencias por documento**. Si no 
hay documentos con esa frecuencia, informa que no se encontraron coincidencias.

Hagamos un test, por ejemplo, con los de frecuencia **43**:

```{r}
test_specific_frequency_summary(43, freq_verbs_summary, list_verbs_text, list_verbs_summary_unique, corpus_ids)
```

Podemos hacer lo mismo con los **títulos**, por ejemplo, con los que tengan **30**
frecuencias y queda lo siguiente:

```{r}
test_specific_frequency_titles(30, freq_verbs_titles, list_verbs_text, list_verbs_title_unique, corpus_ids)
```

