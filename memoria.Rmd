---
title: "Memoria de la pr√°ctica"
output: pdf_document
date: "2024-11-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Grupo 1:

Oliver Arnaldo Anderson Llorens - 
David G√≥mez Mart√≠n - 
Jose Ma - jose.ma@alumnos.upm.es
Joaqu√≠n Daniel Negrete Saab - 
Anton Pogromskyy - anton.pogromskyy@alumnos.upm.es 
Mar√≠a del Mar Sierra Gil - mdm.sierra.gil@alumnos.upm.es

# Lista de tareas de cada uno de los integrantes y el porcentaje de contribuci√≥n:
  - Parte 1: Mar√≠a del Mar (100%)
  - Parte 2: Jose Ma (50%) y Joaqu√≠n Negrete (50%)
  - Parte 3: Anton Pogromskyy (70%) y Oliver Arnaldo (30%)


# Parte 1
## Objetivo

1. Leer un archivo JSON lineal con datos textuales en espa√±ol.
2. Crear un corpus utilizando la librer√≠a `quanteda`.
3. Enriquecer el corpus con variables documentales (docvars).
4. Guardar el corpus en un archivo `.rds`.

## Dise√±o y ejecuci√≥n:
En este primer programa se pide leer el fichero ‚Äúspanish_train.json‚Äù  y crear un corpus quanteda.

El primer paso fue descargar los datos de la p√°gina web de HuggingFace, para posteriormente descomprimir la carpeta obtenida y as√≠ poder usar el fichero ‚Äúspanish_train.json‚Äù.

En el c√≥digo se importar√°n las librer√≠as jsonlite y quanteda para la realizaci√≥n de esta primera parte.
La funci√≥n stream_in() del paquete jsonlile nos permite leer y cargar los datos del fichero previamente descargado en la variable data.

Creamos un corpus y asignamos docvars y el contenido del docvar usando el contenido de data (lectura del fichero jsonl)

Por √∫ltimo lo guardamos en un fichero para su posterior uso: fichero spanish_train.qcorpus.rds

Para ejecutar el c√≥digo puede hacerlo desde aqu√≠
```{r warning=FALSE}
# Comprobar que est√° en el directorio padre del trabajo:
getwd()

# Ejecuci√≥n del fichero
setwd("fuentes")
getwd()
source("train_qcorpus.R")
setwd("../")
```


# Parte 2
## Introducci√≥n
Este documento resume el proceso de an√°lisis de un corpus en espa√±ol mediante el uso de herramientas como `spacy`, `quanteda` y `udpipe`. El objetivo principal fue analizar las estructuras ling√º√≠sticas presentes en los t√≠tulos y res√∫menes del corpus, generando informaci√≥n estad√≠stica sobre el n√∫mero de tokens y su distribuci√≥n.

## Importaci√≥n de Librer√≠as y Datos
Primero, cargamos las librer√≠as necesarias (`spacyr`, `quanteda` y `udpipe`) y los datos del corpus, que incluyen los campos de t√≠tulo y resumen. Tambi√©n cargamos un modelo ling√º√≠stico para el idioma espa√±ol (`spanish-ancora`) compatible con `udpipe`.

## Procesamiento con SpaCy
Utilizamos `spacy` para analizar los campos de t√≠tulo y resumen:

1. Parseamos los textos para extraer informaci√≥n ling√º√≠stica, incluyendo el lema y la categor√≠a gramatical de cada palabra.
2. Filtramos los resultados para eliminar signos de puntuaci√≥n y enfocarnos en los lemas relevantes.

El resultado fue un subconjunto limpio de datos con los campos `doc_id` (identificador del documento) y `lemma` (forma lematizada de las palabras).

## Creaci√≥n de Listas de Tokens
Desarrollamos una funci√≥n personalizada que agrupa los tokens por documento. Esto nos permiti√≥ contar el n√∫mero total de palabras por documento tanto para los t√≠tulos como para los res√∫menes.
Al principio hicimos una primera funci√≥n y parec√≠a que no funcionaba correctamente, puesto que no cog√≠a bien las palabras del √∫ltimo documento y tampoco ten√≠amos en cuenta si le pasamos un dataframe sin valores. Aqu√≠ le muestro la primera funci√≥n:

```{r}
lista_agrupada <- function(dataframe_text_lemma) {
  titulos_unicos <- unique(dataframe_text_lemma$doc_id)
  lista_titulos <- list()
  pos_titulo_unico <- 1
  palabras <- 0
  for (i in 1:nrow(dataframe_text_lemma)) {
    if (dataframe_text_lemma[i, 1] == titulos_unicos[pos_titulo_unico]) {
      palabras <- palabras + 1
    } else if (dataframe_text_lemma[i, 1] != titulos_unicos[pos_titulo_unico]) {
      lista_titulos[titulos_unicos[pos_titulo_unico]] <- palabras
      pos_titulo_unico <- pos_titulo_unico + 1
      palabras <- 1
    }
  }
  return(lista_titulos)
}
```

Luego simplemente le a√±adimos la √∫ltima fila y ya podr√° tener en cuenta el √∫ltimo documento.


Estas listas se usaron posteriormente para generar estad√≠sticas descriptivas.

## Visualizaci√≥n de Resultados
Creamos histogramas para visualizar la distribuci√≥n del n√∫mero de tokens en los t√≠tulos y los res√∫menes:

- Los histogramas de los t√≠tulos mostraron la mayor√≠a de los documentos con menos de 50 tokens.
- Los histogramas de los res√∫menes mostraron una mayor diversidad, con la mayor√≠a de los documentos teniendo menos de 180 tokens.


## Procesamiento con UDPipe
Para complementar el an√°lisis, utilizamos `udpipe` para realizar un an√°lisis gramatical detallado. Este proceso incluy√≥:

1. Anotar los textos del corpus con el modelo `spanish-ancora`.
2. Guardar los resultados en ficheros para optimizar el tiempo de procesamiento.
3. Limpiar los datos eliminando puntuaci√≥n y manteniendo solo los lemas relevantes.


## Comparaci√≥n de Resultados
Se utiliz√≥ la misma funci√≥n para agrupar y contar los tokens en los datos procesados con `udpipe`. Finalmente, generamos histogramas para comparar la distribuci√≥n de tokens en los t√≠tulos y res√∫menes utilizando ambas herramientas.

![Histograma de t√≠tulos y res√∫menes con SpacyR](resultados/histograma_spacy2.png)
![Histograma de t√≠tulos y res√∫menes con UDPipe](resultados/histograma_udpipe2.png)

Como podemos ver, ambos histogramas son casi id√©nticos, se pueden ver peque√±as diferencias como los valores m√°ximos de cada uno de los tipos: el summary con spacy tiene 172 mientras que el de udpipe tiene 171; el titulo con spacy tiene 37 mientras que el de udpipe tiene 39. Estas diferencias son muy peque√±as y casi no se nota en las gr√°ficas por lo que confirmamos que los resultados obtenidos con `udpipe` son consistentes con los obtenidos previamente con `spacy`.

Se puede ver que el n√∫mero de palabras que m√°s abundan en t√≠tulos est√° entre 5 y 20, mientras que en el summary est√°n entre 10 y 40, pudiendo ver adem√°s que hay una asimetr√≠a a la derecha puesto que la mayor√≠a de los datos se encuentran a la izquierda. 

## Ejecuci√≥n del c√≥digo fuente
Si quiere ejecutar el c√≥digo fuente debe de permanecer en el directorio padre del trabajo y ejecutar el siguiente c√≥digo:
```{r warning=FALSE}
# Comprobar que est√° en el directorio padre del trabajo:
getwd()

# Ejecuci√≥n del fichero
setwd("fuentes")
getwd()
source("train_stats.r")
setwd("../")
```



## Prueba
Para llevar a cabo el testing para probar la funci√≥n y la limpieza de los datos. Podemos ejecutar el siguiente c√≥digo para comprobar si los tests se ejecutan. 

```{r warning=FALSE}
# Comprobar que est√° en el directorio padre del trabajo:
getwd()

# Ejecuci√≥n del fichero
setwd("pruebas")
getwd()
source("test_2.R")
setwd("../")
```
Esto es el resultado comentado:

Test 1: Importaci√≥n del corpus
Test passed üò∏

Test2: Prueba de la funci√≥n lista_agrupada. La lista se compone de los dos docs y el resultado debe ser 2 y 3
$doc1
[1] 2

$doc2
[1] 3

[1] 2
[1] 3

Test passed üéä

Test3: Prueba de la funci√≥n lista_agrupada. el dataframe tiene s√≥lo un elemento y se comprueba si da el mismo elemento. El nulo es porque se accede al segundo elemento de la lista el cual no existe
$doc1
[1] 3

[1] 3

NULL
Test passed üò∏

Test4: Prueba de la funci√≥n lista_agrupada con un elemento nulo.

list()
Test passed üò∏

Test5: Probamos si se guardan bien los resultados en un fichero
Test passed ü•≥


Test5: Probamos c√≥mo elimina signos de puntuaci√≥n del dataframe
sample_data <- data.frame(
        doc_id = c("doc1", "doc1", "doc2"),
        lemma = c("palabra1", "palabra2", NA),
        pos = c("NOUN", "PUNCT", "PUNCT")
    )
Adem√°s probamos el acceso al lemma limpio que debe dar "palabra1":

[1] 1
[1] "palabra1"
Test passed ü•≥

Test6: Probamos el funcionamiento del histograma y el resultado es el esperado de los breaks y count.
[1] 0 1 2 3 4 5
[1] 1 2 3 0 0
Test passed üò∏


## Problemas que Tuvimos
El principal inconveniente encontrado fue el tiempo de procesamiento de `udpipe`. Anotar los t√≠tulos y res√∫menes del corpus tom√≥ aproximadamente 30 minutos. Para mitigar este problema, guardamos los resultados procesados en ficheros RDS. Esto nos permiti√≥ cargar los datos r√°pidamente en ejecuciones futuras, reduciendo significativamente el tiempo requerido.

## Conclusiones
Este an√°lisis demostr√≥ la utilidad de herramientas ling√º√≠sticas avanzadas para procesar y analizar textos en espa√±ol. Los histogramas generados permitieron visualizar la estructura del corpus y entender mejor la longitud promedio de los t√≠tulos y res√∫menes.

