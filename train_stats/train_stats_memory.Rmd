---
title: "Resumen de Estadísticas de Corpus"
output: pdf_document
date: "2024-11-22"
---

## Introducción
Este documento resume el proceso de análisis de un corpus en español mediante el uso de herramientas como `spacy`, `quanteda` y `udpipe`. El objetivo principal fue analizar las estructuras lingüísticas presentes en los títulos y resúmenes del corpus, generando información estadística sobre el número de tokens y su distribución.

## Importación de Librerías y Datos
Primero, cargamos las librerías necesarias (`spacyr`, `quanteda` y `udpipe`) y los datos del corpus, que incluyen los campos de título y resumen. También cargamos un modelo lingüístico para el idioma español (`spanish-ancora`) compatible con `udpipe`.

## Procesamiento con SpaCy
Utilizamos `spacy` para analizar los campos de título y resumen:

1. Parseamos los textos para extraer información lingüística, incluyendo el lema y la categoría gramatical de cada palabra.
2. Filtramos los resultados para eliminar signos de puntuación y enfocarnos en los lemas relevantes.

El resultado fue un subconjunto limpio de datos con los campos `doc_id` (identificador del documento) y `lemma` (forma lematizada de las palabras).

## Creación de Listas de Tokens
Desarrollamos una función personalizada que agrupa los tokens por documento. Esto nos permitió contar el número total de palabras por documento tanto para los títulos como para los resúmenes. Estas listas se usaron posteriormente para generar estadísticas descriptivas.

## Visualización de Resultados
Creamos histogramas para visualizar la distribución del número de tokens en los títulos y los resúmenes:

- Los histogramas de los títulos mostraron la mayoría de los documentos con menos de 50 tokens.
- Los histogramas de los resúmenes mostraron una mayor diversidad, con la mayoría de los documentos teniendo menos de 180 tokens.

```r
layout(matrix(1:2, nrow = 1, ncol = 2))
hist(n_tokens_titulos, breaks = seq(0,50,1), ylim = c(0,3500), main = "Histograma títulos", col = "blue", 
     border = "white", xlab = "Número de tokens", ylab = "Frecuencia")
hist(n_tokens_summary, breaks = seq(0,180,2), ylim = c(0,3500), main = "Histograma resúmenes", col = "red",
     border = "white", xlab = "Número de tokens", ylab = "Frecuencia")
```

## Procesamiento con UDPipe
Para complementar el análisis, utilizamos `udpipe` para realizar un análisis gramatical detallado. Este proceso incluyó:

1. Anotar los textos del corpus con el modelo `spanish-ancora`.
2. Guardar los resultados en ficheros para optimizar el tiempo de procesamiento.
3. Limpiar los datos eliminando puntuación y manteniendo solo los lemas relevantes.

```r
saveRDS(data_frame_titulos_udpipe, "data_frame_titulos_udpipe.rds")
data_frame_titulos_udpipe <- readRDS("data_frame_titulos_udpipe.rds")
```

## Comparación de Resultados
Se utilizó la misma función para agrupar y contar los tokens en los datos procesados con `udpipe`. Finalmente, generamos histogramas para comparar la distribución de tokens en los títulos y resúmenes utilizando ambas herramientas.

- Las distribuciones fueron similares a las obtenidas con `spacy`, confirmando la consistencia en el procesamiento.

## Problemas que Tuvimos
El principal inconveniente encontrado fue el tiempo de procesamiento de `udpipe`. Anotar los títulos y resúmenes del corpus tomó aproximadamente 30 minutos. Para mitigar este problema, guardamos los resultados procesados en ficheros RDS. Esto nos permitió cargar los datos rápidamente en ejecuciones futuras, reduciendo significativamente el tiempo requerido.

## Conclusiones
Este análisis demostró la utilidad de herramientas lingüísticas avanzadas para procesar y analizar textos en español. Los histogramas generados permitieron visualizar la estructura del corpus y entender mejor la longitud promedio de los títulos y resúmenes.
